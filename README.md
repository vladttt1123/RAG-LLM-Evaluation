# LLM Evaluation Project

This project is part of the **Udemy course**: [RAG LLM Evaluation AI Test](https://www.udemy.com/course/rag-llm-evaluation-ai-test/).  
It focuses on evaluating various aspects of **Language Learning Models (LLMs)** using different metrics.

---

## ðŸ“‚ Project Structure

- **`conftest.py`** â€“ Handles setup for loading environment variables and defining test fixtures.  
- **`pytest.ini`** â€“ Configuration file for Pytest settings.  
- **`tests/`** â€“ Directory containing test files for different evaluation metrics:  
  - ðŸ“Œ **`ContextPrecision.py`** â€“ Tests the **precision** of context usage in LLM responses.  
  - ðŸ“Œ **`ContextRecall.py`** â€“ Evaluates the **recall** of context used by the LLM.  
  - ðŸ“Œ **`ContextRecallFramework.py`** â€“ Refactored version of **context precision & recall** tests.  
  - ðŸ“Œ **`Faithfulness.py`** â€“ Measures how **faithful** the LLM response is to the source content.  
  - ðŸ“Œ **`RelevanceAndFactual.py`** â€“ Checks **relevancy & factual correctness** of responses.  
  - ðŸ“Œ **`TopicAdherence.py`** â€“ Assesses how well the LLM **adheres to the given topic**.  
  - ðŸ“Œ **`testDataFactory.py`** â€“ Generates **test data** and pushes it to RAGAS.  
  - ðŸ“Œ **`utils.py`** â€“ Utility functions for **loading test data & fetching LLM responses**.  

---

## ðŸ”‘ Environment Variables

Create a `.env` file and add the following environment variables:

```ini
OPENAI_API_KEY=your_openai_api_key
RAGAS_APP_TOKEN=your_ragas_app_token
